{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9332ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ROM_DIR\"] = \"/net/csefiles/xzhanglab/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/AutoROM/roms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca882c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from ale_py import ALEInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e125e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# @title Imports\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# code should work on either, faster on gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# random seeds for reproducability\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb5ab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf14d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Replay Buffer\n",
    "class ReplayBuffer: # why have a replay buffer ? \n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.batch_size = 256\n",
    "\n",
    "    def store(self, state, skill, action, reward, next_state, done):\n",
    "        transitions = list(zip(state, skill, action, reward, next_state, 1 - torch.Tensor(done)))\n",
    "        self.buffer.extend(transitions)\n",
    "\n",
    "    def sample(self):\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        return [torch.stack(e).to(device) for e in zip(*batch)]  # state, skill, action, reward, next_state, not_done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e47139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualization Code\n",
    "import os\n",
    "from gym.wrappers import RecordVideo\n",
    "from IPython.display import Video, display, clear_output\n",
    "\n",
    "def visualize(policy, n_episodes=10, max_steps=1000):\n",
    "    \"\"\"Visualize agent in an Atari environment.\"\"\"\n",
    "\n",
    "    env = gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\")\n",
    "    env = gym.wrappers.RecordVideo(\n",
    "        env,\n",
    "        episode_trigger=lambda num: num % 2 == 0,\n",
    "        video_folder=\"./videos\",\n",
    "        name_prefix=\"pacman-video\",\n",
    "    )\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        episode_over = False\n",
    "\n",
    "        for i in range(max_steps):\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            episode_over = done or truncated\n",
    "            if episode_over:\n",
    "              break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Display the latest video\n",
    "    clear_output(wait=True)\n",
    "    display(Video(\"./rl-video-episode-0.mp4\", embed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3e002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Evaluation Code\n",
    "\n",
    "def evaluate(policy):\n",
    "\n",
    "    # Create environment in rgb_array mode\n",
    "    env = gym.make(\"InvertedPendulum-v5\", reset_noise_scale=0.1, frame_skip=5)\n",
    "\n",
    "    n = 3\n",
    "    mean_duration = 0\n",
    "    for i in range(n):\n",
    "        obs, _ = env.reset()\n",
    "        done, t = False, 0\n",
    "        while not done and t < 200:\n",
    "            with torch.no_grad():\n",
    "                actions = policy(torch.Tensor(obs).to(device)[None, :])[:, 0]\n",
    "            obs, _, done, _, _ = env.step(actions.cpu().numpy())\n",
    "            t += 1\n",
    "\n",
    "        mean_duration += t\n",
    "        print(f\"trial {i+1}/{n} lasted {t*.1:.3f} seconds\")\n",
    "\n",
    "    env.close()\n",
    "    print(f\"\\nmean duration: {(mean_duration * .1 / n):.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58bf4077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2154436), started 1 day, 3:59:38 ago. (Use '!kill 2154436' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-629f6fbed82c07cd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-629f6fbed82c07cd\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f35696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from ale_py import ALEInterface\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import tqdm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "import os\n",
    "from gymnasium.wrappers import RecordVideo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc03e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f82fd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7c6f537668c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90dc29a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%d_%H%M%S\")\n",
    "writer = SummaryWriter(log_dir=f'runs/SAC-Discrete-{timestamp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c16d89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "907411ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_memory():\n",
    "    gc.collect()  # Clean up CPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()            # Release cached GPU memory (PyTorch only)\n",
    "        torch.cuda.ipc_collect()            # Clean up interprocess memory (multi-GPU safe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db013605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "547f1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store experience in a replay buffer, sample from it \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.batch_size = 512\n",
    "\n",
    "    def store(self, state, skill, action, reward, next_state, done):\n",
    "        transitions = list(zip(state, skill, action, reward, next_state, 1 - torch.Tensor(done)))\n",
    "        self.buffer.extend(transitions)\n",
    "\n",
    "    def sample(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            batch = random.choices(self.buffer, k=self.batch_size)\n",
    "            warnings.warn(f\"Requested batch size {self.batch_size} is larger than buffer size \\\n",
    "                 {len(self.buffer)}. Sampling with replacement.\", category=UserWarning)\n",
    "\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "        return [torch.stack(e).to(device) for e in zip(*batch)]  # state, skill, action, reward, next_state, not_done\n",
    "    \n",
    "    def sample_by_skill(self, skill_id, num_samples=128): ## THIS METHOD ASSUMES ONE HOT ENCODED SKILLS\n",
    "        filtered = [transition for transition in self.buffer\n",
    "                    if torch.argmax(transition[1]).item() == skill_id]\n",
    "\n",
    "        if len(filtered) < num_samples:\n",
    "            batch = filtered\n",
    "            warnings.warn(f\"Not enough samples for skill {skill_id}. Sampling with replacement.\", category=UserWarning)\n",
    "        else:\n",
    "            batch = random.sample(filtered, num_samples)\n",
    "\n",
    "        return [torch.stack(e).to(device) for e in zip(*batch)]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75fd17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to encode input pacman images \n",
    "# class CNNEncoder(nn.Module):\n",
    "#     def __init__(self, out_dim):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=8, stride=4),  # (3, 210, 160) → (32, 52, 39)\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, kernel_size=4, stride=2), # → (64, 25, 18)\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, stride=1), # → (64, 23, 16)\n",
    "#             nn.ReLU()\n",
    "#         ).to(device)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(64 * 22 * 16, out_dim),\n",
    "#             nn.ReLU()\n",
    "#         ).to(device)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc(self.conv(x / 255.0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28a2b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRL:\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.n_envs = 32\n",
    "        self.n_steps = 512\n",
    "\n",
    "        self.envs = gym.vector.SyncVectorEnv(\n",
    "            [lambda: gym.make('ALE/MsPacman-v5') for _ in range(self.n_envs)])\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(capacity=buffer_size)\n",
    "\n",
    "    def rollout(self, agent, i, n_skill, encoder):\n",
    "        \"\"\"Collect experience and store it in the replay buffer\"\"\"\n",
    "        encoder = encoder.to(device)\n",
    "        obs, _ = self.envs.reset()\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device).permute(0, 3, 1, 2)# .to(device)\n",
    "        enc_obs = encoder(obs)\n",
    "\n",
    "        env_skills = F.normalize(torch.randn(self.n_envs, n_skill, device=device), dim=1)  # shape: (n_envs, n_skill)\n",
    "        total_rewards = torch.zeros(self.n_envs, device=device)\n",
    "\n",
    "\n",
    "        for step_num in range(self.n_steps):\n",
    "            with torch.no_grad():\n",
    "                actions = agent.get_action(enc_obs, env_skills)\n",
    "\n",
    "            next_obs, rewards, dones, truncateds, _ = self.envs.step(actions.cpu().numpy())\n",
    "            next_obs = torch.tensor(next_obs, dtype=torch.float32, device=device).permute(0, 3, 1, 2)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "            not_done = ~(dones | truncateds)\n",
    "\n",
    "            # Store the transitions\n",
    "            self.replay_buffer.store(obs, env_skills, actions, rewards, next_obs, not_done)\n",
    "            obs = next_obs\n",
    "            with torch.no_grad():\n",
    "                enc_obs = encoder(obs)\n",
    "            total_rewards += rewards\n",
    "\n",
    "            # Resample skills only for environments where episode ended\n",
    "            done_mask = dones | truncateds\n",
    "            if done_mask.any() or step_num % 50 == 0: ## RESAMPLING SKILLS QUITE A LOT :)\n",
    "                new_skills = F.normalize(torch.randn(done_mask.sum(), n_skill, device=device), dim=1)\n",
    "                env_skills[done_mask] = new_skills\n",
    "\n",
    "\n",
    "        writer.add_scalar(\"stats/Rewards\", total_rewards.mean().item() / self.n_steps, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fe1cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(state, encoder): # takes state and passes through CNNEncoder \n",
    "    if state.dim() == 4 and state.shape[1] == 3:\n",
    "        features = encoder(state)\n",
    "    elif state.dim() == 3 and state.shape[0] == 3:\n",
    "        state = state.unsqueeze(0)\n",
    "        features = encoder(state)\n",
    "    elif state.dim() == 2:\n",
    "        features = state\n",
    "    else:\n",
    "        raise ValueError(f'found a state with shape: {state.shape} in get_latent_representationss')\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1164da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a3ed6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillPolicy: # implements discrete SAC \n",
    "    def __init__(self, n_obs, n_skills, n_actions, representation, tau=0.005, lr=3e-4, gamma = 0.99, automatic_entropy_tuning=True, par_alpha=0.2, target_entropy=None):\n",
    "        \n",
    "        \n",
    "        self.alpha = par_alpha # 1.5\n",
    "        self.n_actions = n_actions\n",
    "        self.encoder = representation.encoder\n",
    "        self.representation = representation\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Q1Network\n",
    "        self.q1_net = nn.Sequential(\n",
    "            nn.Linear(n_obs + n_skills, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.n_actions)\n",
    "        ).to(device)\n",
    "        \n",
    "        # Q2Network\n",
    "        self.q2_net = copy.deepcopy(self.q1_net).to(device)\n",
    "\n",
    "        # Policy Network\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(n_obs + n_skills, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.n_actions)\n",
    "        ).to(device)\n",
    "\n",
    "        # Target Q1 Netowrk \n",
    "        self.q1_target_net = copy.deepcopy(self.q1_net).to(device)\n",
    "        \n",
    "        # Target Q2 network\n",
    "        self.q2_target_net = copy.deepcopy(self.q1_net).to(device)\n",
    "\n",
    "        # self.q1_target_net.load_state_dict(self.q1_net.state_dict()) # maybe faster\n",
    "        # self.q2_target_net.load_state_dict(self.q2_net.state_dict())\n",
    "\n",
    "        # Single Q optimizer \n",
    "        self.q_optimizer = Adam(\n",
    "            list(self.q1_net.parameters()) + list(self.q2_net.parameters()), lr=lr\n",
    "        )\n",
    "\n",
    "        # Policy Optimizer \n",
    "        self.policy_optimizer = Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "        # self.gamma = gamma\n",
    "\n",
    "        ###########   add temperature  ############\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "        if automatic_entropy_tuning:\n",
    "            # target entropy = −|A|\n",
    "            self.target_entropy = -self.n_actions if target_entropy is None else target_entropy\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "            self.alpha_optimizer = Adam([self.log_alpha], lr=lr)\n",
    "        else:\n",
    "            self.alpha = par_alpha\n",
    "        ################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############ Selection of the action  #############\n",
    "    def get_policy_distribution(self, states, skills):\n",
    "        states = encode_state(states, self.encoder)\n",
    "        inputs = torch.cat([states, skills], dim=-1)\n",
    "        logits = self.policy(inputs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def get_action(self, states, skills, eval=False):\n",
    "        dist = self.get_policy_distribution(states, skills)\n",
    "        if eval:\n",
    "            return torch.argmax(dist.probs, dim=-1)\n",
    "        return dist.sample()\n",
    "    ###################################################\n",
    "\n",
    "\n",
    "    # def get_entropy(self, states, skills):\n",
    "    #     dist = self.get_policy_distribution(states, skills)\n",
    "    #     return dist.entropy()\n",
    "\n",
    "\n",
    "    def get_q_loss(self, states, actions, rewards, next_states, not_dones, skills):\n",
    "\n",
    "        # ext_reward = rewards.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            phi = self.representation.get_latent_representation(states)  # phi(s) \n",
    "            phi_next = self.representation.get_latent_representation(next_states) # phi (s')\n",
    "\n",
    "            delta_phi = phi - phi_next\n",
    "            # skills = F.normalize(skills, p=2, dim=1)\n",
    "\n",
    "            intrinsic_reward = torch.einsum('bi,bi->b', delta_phi, skills).unsqueeze(-1)\n",
    "            intrinsic_reward = torch.clamp(intrinsic_reward, -10, 10)\n",
    "            # add extrinsic reward ??\n",
    "            \n",
    "            ############## target Q value #######################\n",
    "            next_states = encode_state(next_states, self.encoder)\n",
    "            next_input = torch.cat([next_states, skills], dim=-1)\n",
    "            next_q1 = self.q1_target_net(next_input)\n",
    "            next_q2 = self.q2_target_net(next_input)\n",
    "            next_q = torch.min(next_q1, next_q2)\n",
    "\n",
    "            \n",
    "            next_pi = self.get_policy_distribution(next_states, skills)\n",
    "            log_probs = next_pi.logits.log_softmax(dim=-1)\n",
    "            # next_entropy = next_pi.entropy().unsqueeze(-1)\n",
    "            # next_entropy = next_pi.logits.log_softmax(dim=-1)\n",
    "\n",
    "            if self.automatic_entropy_tuning:\n",
    "                alpha = self.log_alpha.exp()\n",
    "            else:\n",
    "                alpha = self.alpha\n",
    "\n",
    "            next_q_val = (next_pi.probs * (next_q - alpha * log_probs)).sum(dim=-1, keepdim=True)\n",
    "            q_target = intrinsic_reward + (self.gamma * not_dones * next_q_val) # + ext_reward # added extrinisic reward, may not make sense\n",
    "            #####################################################\n",
    "\n",
    "        ############  Q losses ##################\n",
    "        states = encode_state(states, self.encoder)\n",
    "        current_inputs = torch.cat([states, skills], dim=-1)\n",
    "        q1 = self.q1_net(current_inputs).gather(1, actions.long().unsqueeze(-1))\n",
    "        q2 = self.q2_net(current_inputs).gather(1, actions.long().unsqueeze(-1))\n",
    "\n",
    "        loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n",
    "        #######################################\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    #################### policy loss ##################\n",
    "    def get_policy_loss(self, states, skills):\n",
    "        dist = self.get_policy_distribution(states, skills)\n",
    "        probs = dist.probs\n",
    "        log_probs = dist.logits.log_softmax(dim=-1)\n",
    "\n",
    "        states = encode_state(states, self.encoder)\n",
    "\n",
    "        inputs = torch.cat([states, skills], dim=-1)\n",
    "        q1 = self.q1_net(inputs)\n",
    "        q2 = self.q2_net(inputs)\n",
    "        q = torch.min(q1, q2)\n",
    "\n",
    "        # added alpha \n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha = self.log_alpha.exp()\n",
    "        else:\n",
    "            alpha = self.alpha\n",
    "\n",
    "\n",
    "        policy_loss = -(probs * (q - (alpha * log_probs))).sum(dim=1).mean()\n",
    "        return policy_loss\n",
    "    #####################################################\n",
    "\n",
    "    ############### entropy loss ###################\n",
    "    def get_entropy_loss(self, states, skills):\n",
    "        dist = self.get_policy_distribution(states, skills)\n",
    "        # dist.entropy()\n",
    "        probs = dist.probs\n",
    "        log_probs = dist.logits.log_softmax(dim=-1)\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n",
    "        else:\n",
    "            alpha_loss = 0\n",
    "        return alpha_loss\n",
    "    # self.alpha_opt.zero_grad()\n",
    "    #         alpha_loss.backward()\n",
    "    #         self.alpha_opt.step()\n",
    "    ###################################################    \n",
    "\n",
    "\n",
    "    def update(self, replay_buffer, i):\n",
    "\n",
    "      for _ in range(1): # i dont think its good idea to set this too high ?\n",
    "        states, skills, actions, rewards, next_states, not_dones = replay_buffer.sample()\n",
    "        # Compute Q-loss\n",
    "        q_loss = self.get_q_loss(states, actions, rewards, next_states, not_dones, skills)\n",
    "        self.q_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "\n",
    "        # Update the policy\n",
    "        policy_loss = self.get_policy_loss(states, skills)\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "      # Soft update the target networks using Polyak averaging\n",
    "      self.soft_update(self.q1_net, self.q1_target_net)\n",
    "      self.soft_update(self.q2_net, self.q2_target_net)\n",
    "      # entropy = self.get_entropy(states, skills).mean().item()\n",
    "\n",
    "      alpha_loss = self.get_entropy_loss(states, skills)\n",
    "      self.alpha_optimizer.zero_grad()\n",
    "      alpha_loss.backward()\n",
    "      self.alpha_optimizer.step()\n",
    "\n",
    "      writer.add_scalar(\"loss/entropy_loss\", alpha_loss, i)\n",
    "\n",
    "      writer.add_scalar(\"loss/q_loss\", q_loss.item(), i)\n",
    "      writer.add_scalar(\"loss/ - policy loss\", -policy_loss.item(), i)\n",
    "\n",
    "\n",
    "    def soft_update(self, source, target):\n",
    "        \"\"\"Soft update the target network\"\"\"\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45b312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fc79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),  # (3, 210, 160) → (32, 52, 39)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # → (64, 25, 18)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # → (64, 23, 16)\n",
    "            nn.ReLU()\n",
    "        ).to(device)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 22 * 16, out_dim),\n",
    "            nn.ReLU()\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x / 255.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "340d5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RepresentationFunction(nn.Module): # Meat of METRA\n",
    "    def __init__(self, n_obs, n_skill, lr=3e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encode input state \n",
    "        self.encoder = CNNEncoder(out_dim=256) # output of the CNN\n",
    "\n",
    "        # phi network - n_obs is hardcoded to 256 in main\n",
    "        self.representation_func = nn.Sequential(\n",
    "            nn.Linear(n_obs, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_skill)\n",
    "        ).to(device)\n",
    "\n",
    "        # joint optimization over encoder and phi\n",
    "        self.optimizer = Adam(list(self.parameters()) + list(self.encoder.parameters()), lr=1e-4)\n",
    "        \n",
    "        # lagrange mult\n",
    "        self.lambda_param = nn.Parameter(torch.tensor(0.05, requires_grad=True, device=device))\n",
    "        \n",
    "        # lambda optimizer \n",
    "        self.lambda_optimizer = Adam([self.lambda_param], lr=1e-4)\n",
    "        \n",
    "        # threshold for metra constraint \n",
    "        self.epsilon = 0.001 # 1.0\n",
    "\n",
    "\n",
    "    def get_latent_representation(self, state, normalize=False):\n",
    "        features = encode_state(state, self.encoder) # Runs CNNEncoder \n",
    "        x = self.representation_func(features) # phi output \n",
    "        if normalize:\n",
    "            return F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def update(self, replay_buffer, i):\n",
    "        \"\"\"\n",
    "        Updates the representation function φ and the Lagrange multiplier λ\n",
    "        based on skill-consistency and distance constraints.\n",
    "        \"\"\"\n",
    "        for _ in range(50): # 10\n",
    "\n",
    "          state, skill, action, reward, next_state, not_done  = replay_buffer.sample()\n",
    "\n",
    "          current_representations = self.get_latent_representation(state)    # φ(s)\n",
    "          next_representations = self.get_latent_representation(next_state)  # φ(s')\n",
    "\n",
    "          # Consistency loss term\n",
    "          consistency_term = torch.einsum('bi,bi->b', (next_representations - current_representations), skill)\n",
    "\n",
    "          # Distance penalty term (to enforce norm constraints)\n",
    "          diff_norm_squared = torch.norm(current_representations - next_representations, dim=1) ** 2\n",
    "          penalty_term = torch.minimum(\n",
    "              torch.tensor(self.epsilon, device=diff_norm_squared.device),\n",
    "              (1.0 - diff_norm_squared).clone()\n",
    "          )\n",
    "\n",
    "          # Representation loss\n",
    "          representation_loss = -(consistency_term + self.lambda_param * penalty_term).mean()\n",
    "\n",
    "          # Lambda loss (only on penalty term)\n",
    "          lambda_loss = (self.lambda_param * penalty_term.detach()).mean()\n",
    "\n",
    "          # Backprop: update \\phi\n",
    "          self.optimizer.zero_grad()\n",
    "          representation_loss.backward(retain_graph=True)\n",
    "          self.optimizer.step()\n",
    "\n",
    "          # Backprop: update \\lambda\n",
    "          self.lambda_optimizer.zero_grad()\n",
    "          lambda_loss.backward()\n",
    "          self.lambda_optimizer.step()\n",
    "          with torch.no_grad():\n",
    "                self.lambda_param.clamp_(min=0.0)\n",
    "\n",
    "        writer.add_scalar(\"loss/representation_loss\", representation_loss.item(), i)\n",
    "        writer.add_scalar(\"loss/  lambda_loss\", lambda_loss.item(), i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc65456",
   "metadata": {},
   "source": [
    "## not checkedn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09f4c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_heatmap(frames, save_path):\n",
    "    heatmap = None\n",
    "    for frame in frames:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        # naive thresholding to isolate the player\n",
    "        _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "        if heatmap is None:\n",
    "            heatmap = np.zeros_like(thresh, dtype=np.float32)\n",
    "        heatmap += thresh.astype(np.float32)\n",
    "\n",
    "    # Normalize and save heatmap\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(heatmap, cmap='hot', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "def visualize_representation(replay_buffer, representation, n_samples=1000, folder='.', n_skills=10):\n",
    "    skill_data = []\n",
    "    for skill_id in range(n_skills):\n",
    "        samples = replay_buffer.sample_by_skill(skill_id, num_samples=max(int(n_samples/10), 100))\n",
    "        skill_data.append(samples)\n",
    "\n",
    "    # Combine all samples\n",
    "    states = torch.cat([s[0] for s in skill_data], dim=0)\n",
    "    skills = torch.cat([s[1] for s in skill_data], dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        skill_ids = torch.argmax(skills, dim=1)  # assume Gaussian skills aren't passed here!\n",
    "\n",
    "        phis = representation.get_latent_representation(states).cpu().numpy()\n",
    "        skill_ids_np = skill_ids.cpu().numpy()\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=10)\n",
    "    tsne_result = tsne.fit_transform(phis)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for skill_id in range(n_skills):\n",
    "        idx = skill_ids_np == skill_id\n",
    "        if np.sum(idx) == 0:\n",
    "            continue\n",
    "        plt.scatter(tsne_result[idx, 0], tsne_result[idx, 1], label=f'Skill {skill_id}', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.title('t-SNE of Representation φ(s)')\n",
    "    plt.savefig(f\"{folder}/representation_tsne.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate_skills(env_name, policy, representation, global_step, writer=None, n_skills=5, steps_per_skill=512, video_dir='skill_videos'):\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "    temp_buffer = ReplayBuffer(capacity=steps_per_skill * n_skills)\n",
    "    temp_buffer.batch_size = 512\n",
    "\n",
    "    for skill_id in range(n_skills): # For every z vector, I will perform ONLY the action related to that skill and get the reward\n",
    "        env = gym.make(env_name, render_mode='rgb_array')\n",
    "        obs, _ = env.reset()\n",
    "        # obs = torch.tensor(obs, dtype=torch.float32, device=device).view(1, -1) ## NO need to flatten CNN encoder in representation function will handle it\n",
    "        obs = torch.unsqueeze(torch.tensor(obs, dtype=torch.float32, device=device), 0).permute(0, 3, 1, 2)\n",
    "\n",
    "        # USING ONE HOT ENCODED SKILLS HERE SO THAT WE CAN FIND THE DIFFERENT SKILLS SEPARATELY\n",
    "        skill = torch.zeros(1, n_skills, device=device)\n",
    "        skill[0, skill_id] = 1.0  # One-hot vector for evaluation\n",
    "\n",
    "\n",
    "        frames = []\n",
    "        total_intrinsic_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while step_count < steps_per_skill:\n",
    "            with torch.no_grad():\n",
    "                action = policy.get_action(obs, skill, eval=True).item()\n",
    "            next_obs, _, terminated, truncated, _ = env.step(action)\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "\n",
    "            # next_obs_tensor = torch.tensor(next_obs, dtype=torch.float32, device=device).view(1, -1)\n",
    "            next_obs_tensor = torch.unsqueeze(torch.tensor(next_obs, dtype=torch.float32, device=device), 0).permute(0, 3, 1, 2) ## Encoding doesn't matter because get_latent_representation encodes\n",
    "            delta_phi = representation.get_latent_representation(next_obs_tensor) - representation.get_latent_representation(obs)\n",
    "            intrinsic_reward = torch.einsum('bi,bi->b', delta_phi, skill).item()\n",
    "            temp_buffer.store(obs, skill, torch.tensor([action]), torch.tensor([intrinsic_reward]), next_obs_tensor, torch.tensor([~(terminated or truncated)]))\n",
    "            total_intrinsic_reward += intrinsic_reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            obs = next_obs_tensor\n",
    "            step_count += 1\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        video_path = os.path.join(video_dir, f\"skill_{skill_id}.mp4\")\n",
    "        imageio.mimsave(video_path, frames, fps=30)\n",
    "\n",
    "        writer.add_scalar(\"representation/mean_delta_phi_norm\", delta_phi.norm(dim=1).item(), global_step)\n",
    "\n",
    "        heatmap_path = os.path.join(video_dir, f\"skill_{skill_id}_heatmap.png\")\n",
    "        # compute_heatmap(frames, heatmap_path)\n",
    "\n",
    "        del frames\n",
    "        torch.cuda.empty_cache()  \n",
    "        # print(f\"Skill {skill_id}: steps = {step_count}, intrinsic reward = {total_intrinsic_reward:.2f}, saved to {video_path}\")\n",
    "        if writer:\n",
    "            writer.add_scalar(f\"eval/intrinsic_reward_skill_{skill_id}\", total_intrinsic_reward, global_step)\n",
    "            writer.add_scalar(f\"eval/episode_length_skill_{skill_id}\", step_count, global_step)\n",
    "    \n",
    "    visualize_representation(temp_buffer, representation, folder=video_dir, n_skills=n_skills)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac4c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c41c70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n",
      "/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2\n",
      "EPOCH: 3\n",
      "EPOCH: 4\n",
      "EPOCH: 5\n",
      "EPOCH: 6\n",
      "EPOCH: 7\n",
      "EPOCH: 8\n",
      "EPOCH: 9\n",
      "EPOCH: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12\n",
      "EPOCH: 13\n",
      "EPOCH: 14\n",
      "EPOCH: 15\n",
      "EPOCH: 16\n",
      "EPOCH: 17\n",
      "EPOCH: 18\n",
      "EPOCH: 19\n",
      "EPOCH: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 22\n",
      "EPOCH: 23\n",
      "EPOCH: 24\n",
      "EPOCH: 25\n",
      "EPOCH: 26\n",
      "EPOCH: 27\n",
      "EPOCH: 28\n",
      "EPOCH: 29\n",
      "EPOCH: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 32\n",
      "EPOCH: 33\n",
      "EPOCH: 34\n",
      "EPOCH: 35\n",
      "EPOCH: 36\n",
      "EPOCH: 37\n",
      "EPOCH: 38\n",
      "EPOCH: 39\n",
      "EPOCH: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 42\n",
      "EPOCH: 43\n",
      "EPOCH: 44\n",
      "EPOCH: 45\n",
      "EPOCH: 46\n",
      "EPOCH: 47\n",
      "EPOCH: 48\n",
      "EPOCH: 49\n",
      "EPOCH: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 52\n",
      "EPOCH: 53\n",
      "EPOCH: 54\n",
      "EPOCH: 55\n",
      "EPOCH: 56\n",
      "EPOCH: 57\n",
      "EPOCH: 58\n",
      "EPOCH: 59\n",
      "EPOCH: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 62\n",
      "EPOCH: 63\n",
      "EPOCH: 64\n",
      "EPOCH: 65\n",
      "EPOCH: 66\n",
      "EPOCH: 67\n",
      "EPOCH: 68\n",
      "EPOCH: 69\n",
      "EPOCH: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 72\n",
      "EPOCH: 73\n",
      "EPOCH: 74\n",
      "EPOCH: 75\n",
      "EPOCH: 76\n",
      "EPOCH: 77\n",
      "EPOCH: 78\n",
      "EPOCH: 79\n",
      "EPOCH: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 82\n",
      "EPOCH: 83\n",
      "EPOCH: 84\n",
      "EPOCH: 85\n",
      "EPOCH: 86\n",
      "EPOCH: 87\n",
      "EPOCH: 88\n",
      "EPOCH: 89\n",
      "EPOCH: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 92\n",
      "EPOCH: 93\n",
      "EPOCH: 94\n",
      "EPOCH: 95\n",
      "EPOCH: 96\n",
      "EPOCH: 97\n",
      "EPOCH: 98\n",
      "EPOCH: 99\n",
      "EPOCH: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 102\n",
      "EPOCH: 103\n",
      "EPOCH: 104\n",
      "EPOCH: 105\n",
      "EPOCH: 106\n",
      "EPOCH: 107\n",
      "EPOCH: 108\n",
      "EPOCH: 109\n",
      "EPOCH: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391280/746216923.py:131: UserWarning: Using a target size (torch.Size([512, 512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 112\n",
      "EPOCH: 113\n",
      "EPOCH: 114\n",
      "EPOCH: 115\n",
      "EPOCH: 116\n",
      "EPOCH: 117\n",
      "EPOCH: 118\n",
      "EPOCH: 119\n",
      "EPOCH: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 44.45 GiB of which 1.75 MiB is free. Process 2191234 has 418.00 MiB memory in use. Including non-PyTorch memory, this process has 6.92 GiB memory in use. Process 2394182 has 536.00 MiB memory in use. Process 2404824 has 18.14 GiB memory in use. Process 2409193 has 18.40 GiB memory in use. Of the allocated memory 5.69 GiB is allocated by PyTorch, and 935.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m policy\u001b[38;5;241m.\u001b[39mupdate(drl\u001b[38;5;241m.\u001b[39mreplay_buffer, epoch)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m## EVALUATING SKILL\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mevaluate_skills\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mALE/MsPacman-v5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepresentation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_skills\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_skill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfolder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/video_eval_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m## ONLY doing for the first 3 skills\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m## EVALUATING REPRESENTATION FUNCTION\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# visualize_representation(replay_buffer=drl.replay_buffer,\\\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m#      representation=representation, folder=f\"{folder}/video_eval_{epoch}\") ## DOING THIS IN SkILL EVALUATION WHICH DOES ONE HOT ENCODED SKILLS\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# writer.add_image(\"eval/tSNE\", \\\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m#      torch.tensor(np.array(Image.open(f\"{folder}/video_eval_{epoch}/representation_tsne.png\"))).permute(2, 0, 1), epoch)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[0;32mIn[23], line 79\u001b[0m, in \u001b[0;36mevaluate_skills\u001b[0;34m(env_name, policy, representation, global_step, writer, n_skills, steps_per_skill, video_dir)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# next_obs_tensor = torch.tensor(next_obs, dtype=torch.float32, device=device).view(1, -1)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m next_obs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(torch\u001b[38;5;241m.\u001b[39mtensor(next_obs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice), \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m## Encoding doesn't matter because get_latent_representation encodes\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m delta_phi \u001b[38;5;241m=\u001b[39m representation\u001b[38;5;241m.\u001b[39mget_latent_representation(next_obs_tensor) \u001b[38;5;241m-\u001b[39m \u001b[43mrepresentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_latent_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m intrinsic_reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi,bi->b\u001b[39m\u001b[38;5;124m'\u001b[39m, delta_phi, skill)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     81\u001b[0m temp_buffer\u001b[38;5;241m.\u001b[39mstore(obs, skill, torch\u001b[38;5;241m.\u001b[39mtensor([action]), torch\u001b[38;5;241m.\u001b[39mtensor([intrinsic_reward]), next_obs_tensor, torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m~\u001b[39m(terminated \u001b[38;5;129;01mor\u001b[39;00m truncated)]))\n",
      "Cell \u001b[0;32mIn[22], line 31\u001b[0m, in \u001b[0;36mRepresentationFunction.get_latent_representation\u001b[0;34m(self, state, normalize)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_latent_representation\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 31\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mencode_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Runs CNNEncoder \u001b[39;00m\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_func(features) \u001b[38;5;66;03m# phi output \u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize:\n",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m, in \u001b[0;36mencode_state\u001b[0;34m(state, encoder)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_state\u001b[39m(state, encoder): \u001b[38;5;66;03m# takes state and passes through CNNEncoder \u001b[39;00m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m         features \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m      5\u001b[0m         state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m, in \u001b[0;36mCNNEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 44.45 GiB of which 1.75 MiB is free. Process 2191234 has 418.00 MiB memory in use. Including non-PyTorch memory, this process has 6.92 GiB memory in use. Process 2394182 has 536.00 MiB memory in use. Process 2404824 has 18.14 GiB memory in use. Process 2409193 has 18.40 GiB memory in use. Of the allocated memory 5.69 GiB is allocated by PyTorch, and 935.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env = gym.make('ALE/MsPacman-v5')\n",
    "# n_obs = env.observation_space.shape[0] * env.observation_space.shape[1] * env.observation_space.shape[2]\n",
    "n_obs = 256 # Encoded dimension\n",
    "n_actions = env.action_space.n\n",
    "n_skill = 10 # 0 - NOOP, 1 - UP, 2 - LEFT, 3 - RIGHT, 4 - DOWN\n",
    "\n",
    "representation = RepresentationFunction(n_obs, n_skill)\n",
    "policy = SkillPolicy(n_obs, n_skill, n_actions, representation)\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "chkpt_path = ''\n",
    "if len(chkpt_path) > 0:\n",
    "    if not os.path.exists(chkpt_path):\n",
    "        raise ValueError(\"Load folder path doesn't exist\")\n",
    "    checkpoint = torch.load(chkpt_path)\n",
    "    policy.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "    policy.q1_net.load_state_dict(checkpoint['q1_state_dict'])\n",
    "    policy.q2_net.load_state_dict(checkpoint['q2_state_dict'])\n",
    "    policy.q1_target_net.load_state_dict(checkpoint['q1_target_state_dict'])\n",
    "    policy.q2_target_net.load_state_dict(checkpoint['q2_target_state_dict'])\n",
    "    policy.policy_optimizer.load_state_dict(checkpoint['policy_optimizer_state_dict'])\n",
    "    policy.q_optimizer.load_state_dict(checkpoint['q_optimizer_state_dict'])\n",
    "\n",
    "    representation.representation_func.load_state_dict(checkpoint['representation_state_dict'])\n",
    "    representation.optimizer.load_state_dict(checkpoint['representation_optimizer_state_dict'])\n",
    "    representation.lambda_param = torch.tensor(checkpoint['lambda_param'], requires_grad=True)\n",
    "    representation.lambda_optimizer.load_state_dict(checkpoint['lambda_optimizer_state_dict'])\n",
    "\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 551\n",
    "\n",
    "drl = DRL(buffer_size = 10_000)\n",
    "\n",
    "if not os.path.exists(f\"./model/{timestamp}\"):\n",
    "    os.makedirs(f\"./model/{timestamp}\")\n",
    "\n",
    "folder = f'./model/{timestamp}'\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + start_epoch):\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    drl.rollout(policy, epoch, n_skill, representation.encoder)\n",
    "    representation.update(drl.replay_buffer, epoch)\n",
    "    policy.update(drl.replay_buffer, epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        ## EVALUATING SKILL\n",
    "        evaluate_skills('ALE/MsPacman-v5', policy, representation,\\\n",
    "                epoch, writer=writer, n_skills=n_skill, video_dir=f'{folder}/video_eval_{epoch}') ## ONLY doing for the first 3 skills\n",
    "\n",
    "        ## EVALUATING REPRESENTATION FUNCTION\n",
    "        # visualize_representation(replay_buffer=drl.replay_buffer,\\\n",
    "        #      representation=representation, folder=f\"{folder}/video_eval_{epoch}\") ## DOING THIS IN SkILL EVALUATION WHICH DOES ONE HOT ENCODED SKILLS\n",
    "        # writer.add_image(\"eval/tSNE\", \\\n",
    "        #      torch.tensor(np.array(Image.open(f\"{folder}/video_eval_{epoch}/representation_tsne.png\"))).permute(2, 0, 1), epoch)\n",
    "\n",
    "        gc.collect()\n",
    "    cleanup_memory()\n",
    "\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'policy_state_dict': policy.policy.state_dict(),\n",
    "    'q1_state_dict': policy.q1_net.state_dict(),\n",
    "    'q2_state_dict': policy.q2_net.state_dict(),\n",
    "    'q1_target_state_dict': policy.q1_target_net.state_dict(),\n",
    "    'q2_target_state_dict': policy.q2_target_net.state_dict(),\n",
    "    'policy_optimizer_state_dict': policy.policy_optimizer.state_dict(),\n",
    "    'q_optimizer_state_dict': policy.q_optimizer.state_dict(),\n",
    "\n",
    "    'representation_state_dict': representation.representation_func.state_dict(),\n",
    "    'representation_optimizer_state_dict': representation.optimizer.state_dict(),\n",
    "    'lambda_param': representation.lambda_param.detach().item(),\n",
    "    'lambda_optimizer_state_dict': representation.lambda_optimizer.state_dict(),\n",
    "}, f\"./model/{timestamp}/checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a93312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a9bad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76b001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "drl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
