{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9332ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"ROM_DIR\"] = \"/net/csefiles/xzhanglab/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/AutoROM/roms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca882c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from ale_py import ALEInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e125e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# @title Imports\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# code should work on either, faster on gpu\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# random seeds for reproducability\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf4077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2154436), started 2 days, 1:04:01 ago. (Use '!kill 2154436' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-629f6fbed82c07cd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-629f6fbed82c07cd\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from ale_py import ALEInterface\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import tqdm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "import os\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82fd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x76844cd145e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc29a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%d_%H%M%S\")\n",
    "writer = SummaryWriter(log_dir=f'runs/SAC-Discrete-{timestamp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16d89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "ale = ALEInterface()\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907411ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_memory():\n",
    "    gc.collect()  # Clean up CPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()            # Release cached GPU memory (PyTorch only)\n",
    "        torch.cuda.ipc_collect()            # Clean up interprocess memory (multi-GPU safe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db013605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store experience in a replay buffer, sample from it \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.batch_size = 256 # 512\n",
    "\n",
    "    def store(self, state, skill, action, reward, next_state, done):\n",
    "        transitions = list(zip(state, skill, action, reward, next_state, 1 - torch.Tensor(done)))\n",
    "        self.buffer.extend(transitions)\n",
    "\n",
    "    def sample(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            batch = random.choices(self.buffer, k=self.batch_size)\n",
    "            warnings.warn(f\"Requested batch size {self.batch_size} is larger than buffer size \\\n",
    "                 {len(self.buffer)}. Sampling with replacement.\", category=UserWarning)\n",
    "\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, self.batch_size)\n",
    "        \n",
    "        states, skills, actions, rewards, next_states, not_dones = zip(*batch)\n",
    "\n",
    "        # stack and move to device\n",
    "        states      = torch.stack(states,     dim=0).to(device)\n",
    "        skills      = torch.stack(skills,     dim=0).to(device)\n",
    "        actions     = torch.stack(actions,    dim=0).to(device)\n",
    "        rewards     = torch.stack(rewards,    dim=0).to(device)\n",
    "        next_states = torch.stack(next_states,dim=0).to(device)\n",
    "        not_done    = torch.stack(not_dones,  dim=0).to(device).unsqueeze(-1)  # [B,1] # .\n",
    "\n",
    "        return states, skills, actions, rewards, next_states, not_done\n",
    "\n",
    "        # return [torch.stack(e).to(device) for e in zip(*batch)]  # state, skill, action, reward, next_state, not_done\n",
    "    \n",
    "    def sample_by_skill(self, skill_id, num_samples=128): ## THIS METHOD ASSUMES ONE HOT ENCODED SKILLS\n",
    "        filtered = [transition for transition in self.buffer\n",
    "                    if torch.argmax(transition[1]).item() == skill_id]\n",
    "\n",
    "        if len(filtered) < num_samples:\n",
    "            # batch = filtered\n",
    "            batch = random.choices(filtered, k=num_samples)\n",
    "            warnings.warn(f\"Not enough samples for skill {skill_id}. Sampling with replacement.\", category=UserWarning)\n",
    "        else:\n",
    "            batch = random.sample(filtered, num_samples)\n",
    "\n",
    "        states, skills, actions, rewards, next_states, not_dones = zip(*batch)\n",
    "        states      = torch.stack(states,     dim=0).to(device)\n",
    "        skills      = torch.stack(skills,     dim=0).to(device)\n",
    "        actions     = torch.stack(actions,    dim=0).to(device)\n",
    "        rewards     = torch.stack(rewards,    dim=0).to(device)\n",
    "        next_states = torch.stack(next_states,dim=0).to(device)\n",
    "        not_done    = torch.stack(not_dones,  dim=0).to(device).unsqueeze(-1) # .to(self.device)\n",
    "\n",
    "        return states, skills, actions, rewards, next_states, not_done\n",
    "\n",
    "        # return [torch.stack(e).to(device) for e in zip(*batch)]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to encode input pacman images \n",
    "# class CNNEncoder(nn.Module):\n",
    "#     def __init__(self, out_dim):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=8, stride=4),  # (3, 210, 160) → (32, 52, 39)\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, kernel_size=4, stride=2), # → (64, 25, 18)\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, stride=1), # → (64, 23, 16)\n",
    "#             nn.ReLU()\n",
    "#         ).to(device)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(64 * 22 * 16, out_dim),\n",
    "#             nn.ReLU()\n",
    "#         ).to(device)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc(self.conv(x / 255.0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DRL:\n",
    "#     def __init__(self, buffer_size = 10000):\n",
    "#         self.n_envs = 32\n",
    "#         self.episodes_per_epoch = 8 # 400 \n",
    "#         # self.n_steps =  512\n",
    "\n",
    "#         self.envs = gym.vector.SyncVectorEnv(\n",
    "#             [lambda: gym.make('ALE/MsPacman-v5') for _ in range(self.n_envs)])\n",
    "\n",
    "#         self.replay_buffer = ReplayBuffer(capacity=buffer_size)\n",
    "\n",
    "#     def rollout(self, agent, epoch, n_skill, encoder): # one episode \n",
    "#         \"\"\"Collect experience and store it in the replay buffer\"\"\"\n",
    "#         encoder = encoder.to(device)\n",
    "        \n",
    "#         # obs, _ = self.envs.reset()\n",
    "#         # obs = torch.tensor(obs, dtype=torch.float32, device=device).permute(0, 3, 1, 2)# .to(device)\n",
    "#         # enc_obs = encoder(obs)\n",
    "\n",
    "#         obs_np, _ = self.envs.reset()\n",
    "#         obs       = torch.tensor(obs_np, dtype=torch.float32).permute(0,3,1,2).to(device)\n",
    "#         with torch.no_grad():\n",
    "#             enc_obs = encoder(obs)\n",
    "#         # env_skills = F.normalize(torch.randn(self.n_envs, n_skill, device=device), dim=1)  # shape: (n_envs, n_skill)\n",
    "#         # total_rewards = torch.zeros(self.n_envs, device=device)\n",
    "\n",
    "#         skill_ids  = torch.randint(0, n_skill, (self.n_envs,), device=device)\n",
    "#         env_skills = F.one_hot(skill_ids, n_skill).float().to(device)\n",
    "        \n",
    "#         # tracking\n",
    "#         episodes_done   = 0\n",
    "#         episode_returns = torch.zeros(self.n_envs, device=device)\n",
    "#         all_episode_returns = []\n",
    "        \n",
    "#         # for step_num in range(self.n_steps): # steps in the trajectory \n",
    "#         #     with torch.no_grad():\n",
    "#         #         actions = agent.get_action(enc_obs, env_skills)\n",
    "\n",
    "#         #     next_obs, rewards, dones, truncateds, _ = self.envs.step(actions.cpu().numpy())\n",
    "#         #     next_obs = torch.tensor(next_obs, dtype=torch.float32, device=device).permute(0, 3, 1, 2)\n",
    "#         #     rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "#         #     not_done = ~(dones | truncateds)\n",
    "\n",
    "#         #     # Store the transitions\n",
    "#         #     self.replay_buffer.store(obs, env_skills, actions, rewards, next_obs, not_done)\n",
    "#         #     obs = next_obs\n",
    "#         #     with torch.no_grad():\n",
    "#         #         enc_obs = encoder(obs)\n",
    "#         #     total_rewards += rewards\n",
    "\n",
    "#         #     # Resample skills only for environments where episode ended\n",
    "#         #     done_mask = dones | truncateds\n",
    "#         #     if done_mask.any() or step_num % 1000 == 0: ## RESAMPLING SKILLS QUITE A LOT :)\n",
    "#         #         new_skills = F.normalize(torch.randn(done_mask.sum(), n_skill, device=device), dim=1)\n",
    "#         #         env_skills[done_mask] = new_skills\n",
    "            \n",
    "#         #     # Resample skill at every iteration\n",
    "#         #     # new_skills = F.normalize(torch.randn(self.n_envs, n_skill, device=device), dim=1)\n",
    "#         #     # env_skills = new_skills\n",
    "        \n",
    "#         while episodes_done < self.episodes_per_epoch:\n",
    "#             # select and step\n",
    "#             with torch.no_grad():\n",
    "#                 actions = agent.get_action(enc_obs, env_skills)\n",
    "#             next_obs, rewards, dones, truncs, _ = self.envs.step(actions.cpu().numpy())\n",
    "#             next_obs = torch.tensor(next_obs, dtype=torch.float32, device=device).permute(0,3,1,2)\n",
    "#             rewards  = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "#             not_done = ~(dones | truncs)\n",
    "\n",
    "#             # store transitions\n",
    "#             self.replay_buffer.store(obs, env_skills, actions, rewards, next_obs, not_done)\n",
    "\n",
    "#             # accumulate returns\n",
    "#             episode_returns += rewards\n",
    "\n",
    "#             # detect finished envs\n",
    "#             done_mask = dones | truncs\n",
    "\n",
    "#             # done_mask\n",
    "#             if done_mask.any():\n",
    "#                 # record returns for each finished env\n",
    "#                 finished_idxs = torch.tensor(done_mask).to(bool).nonzero(as_tuple=True)[0]\n",
    "#                 for idx in finished_idxs:\n",
    "#                     all_episode_returns.append(episode_returns[idx].item())\n",
    "#                 episodes_done += finished_idxs.size(0)\n",
    "\n",
    "#                 # reset those envs\n",
    "#                 # (SyncVectorEnv only supports full reset, so we reset all and copy back)\n",
    "#                 new_obs, _ = self.envs.reset()\n",
    "#                 new_obs = torch.tensor(new_obs, dtype=torch.float32, device=device).permute(0,3,1,2)\n",
    "#                 obs[done_mask] = new_obs[done_mask]\n",
    "\n",
    "#                 # sample new skills for those envs\n",
    "#                 # new_ids = torch.randint(0, n_skill, (finished_idxs.size(0),), device=self.device)\n",
    "#                 # skill_ids[done_mask] = new_ids\n",
    "#                 # env_skills = F.one_hot(skill_ids, n_skill).float()\n",
    "\n",
    "#                 # zero their return counters\n",
    "#                 episode_returns[done_mask] = 0.0\n",
    "#                 # re-encode only reset entries\n",
    "#                 with torch.no_grad():\n",
    "#                     enc_obs = encoder(obs)\n",
    "#                 del next_obs, rewards, dones, truncs, not_done, actions\n",
    "#                 torch.cuda.empty_cache()\n",
    "\n",
    "#             else:\n",
    "#                 obs    = next_obs\n",
    "#                 with torch.no_grad():    \n",
    "#                     enc_obs = encoder(obs)\n",
    "#                 del next_obs, rewards, dones, truncs, not_done, actions\n",
    "#                 torch.cuda.empty_cache()\n",
    "        \n",
    "#         avg_ret = sum(all_episode_returns) / len(all_episode_returns)\n",
    "#         writer.add_scalar(\"stats/Rewards\", avg_ret, epoch)\n",
    "#         # writer.add_scalar(\"stats/Rewards\", total_rewards.mean().item() / self.n_steps, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb723c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DRL:\n",
    "    \n",
    "#     def rollout(self, agent, epoch, n_skill, encoder, episodes_per_epoch=4):\n",
    "#         \"\"\"\n",
    "#         Collect exactly `episodes_per_epoch` full episodes (across the n_envs envs),\n",
    "#         store transitions in the shared replay buffer, and log avg return.\n",
    "#         \"\"\"\n",
    "#         encoder = encoder.to(self.device)\n",
    "#         # initial reset\n",
    "#         obs, _ = self.envs.reset()\n",
    "#         obs = torch.tensor(obs, dtype=torch.float32, device=self.device).permute(0,3,1,2)\n",
    "#         enc_obs = encoder(obs)\n",
    "\n",
    "#         # one skill per env for current episode\n",
    "#         skill_ids  = torch.randint(0, n_skill, (self.n_envs,), device=self.device)\n",
    "#         env_skills = F.one_hot(skill_ids, n_skill).float()\n",
    "\n",
    "#         # tracking\n",
    "#         episodes_done   = 0\n",
    "#         episode_returns = torch.zeros(self.n_envs, device=self.device)\n",
    "#         all_episode_returns = []\n",
    "\n",
    "#         while episodes_done < episodes_per_epoch:\n",
    "#             # select and step\n",
    "#             with torch.no_grad():\n",
    "#                 actions = agent.get_action(enc_obs, env_skills)\n",
    "#             next_obs, rewards, dones, truncs, _ = self.envs.step(actions.cpu().numpy())\n",
    "#             next_obs = torch.tensor(next_obs, dtype=torch.float32, device=self.device).permute(0,3,1,2)\n",
    "#             rewards  = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "#             not_done = ~(dones | truncs)\n",
    "\n",
    "#             # store transitions\n",
    "#             self.replay_buffer.store(obs, env_skills, actions, rewards, next_obs, not_done)\n",
    "\n",
    "#             # accumulate returns\n",
    "#             episode_returns += rewards\n",
    "\n",
    "#             # detect finished envs\n",
    "#             done_mask = dones | truncs\n",
    "#             if done_mask.any():\n",
    "#                 # record returns for each finished env\n",
    "#                 finished_idxs = done_mask.nonzero(as_tuple=True)[0]\n",
    "#                 for idx in finished_idxs:\n",
    "#                     all_episode_returns.append(episode_returns[idx].item())\n",
    "#                 episodes_done += finished_idxs.size(0)\n",
    "\n",
    "#                 # reset those envs\n",
    "#                 # (SyncVectorEnv only supports full reset, so we reset all and copy back)\n",
    "#                 new_obs, _ = self.envs.reset()\n",
    "#                 new_obs = torch.tensor(new_obs, dtype=torch.float32, device=self.device).permute(0,3,1,2)\n",
    "#                 obs[done_mask] = new_obs[done_mask]\n",
    "\n",
    "#                 # sample new skills for those envs\n",
    "#                 new_ids = torch.randint(0, n_skill, (finished_idxs.size(0),), device=self.device)\n",
    "#                 skill_ids[done_mask] = new_ids\n",
    "#                 env_skills = F.one_hot(skill_ids, n_skill).float()\n",
    "\n",
    "#                 # zero their return counters\n",
    "#                 episode_returns[done_mask] = 0.0\n",
    "#                 # re-encode only reset entries\n",
    "#                 enc_obs = encoder(obs)\n",
    "\n",
    "#             else:\n",
    "#                 obs    = next_obs\n",
    "#                 enc_obs = encoder(obs)\n",
    "\n",
    "#         # log average return over the episodes we just finished\n",
    "#         avg_return = sum(all_episode_returns) / len(all_episode_returns)\n",
    "#         writer.add_scalar(\"stats/episode_return\", avg_return, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75742b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRL:\n",
    "    def __init__(self, buffer_size = 100000):\n",
    "        self.n_envs = 32\n",
    "        self.n_steps = 512\n",
    "\n",
    "        self.envs = gym.vector.SyncVectorEnv(\n",
    "            [lambda: gym.make('ALE/MsPacman-v5') for _ in range(self.n_envs)])\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(capacity=buffer_size)\n",
    "\n",
    "    def rollout(self, agent, i, n_skill, encoder):\n",
    "        \"\"\"Collect experience and store it in the replay buffer\"\"\"\n",
    "        encoder = encoder.to(device)\n",
    "        obs, _ = self.envs.reset()\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device).permute(0, 3, 1, 2)# .to(device)\n",
    "        enc_obs = encoder(obs)\n",
    "\n",
    "        env_skills = F.normalize(torch.randn(self.n_envs, n_skill, device=device), dim=1)  # shape: (n_envs, n_skill)\n",
    "        total_rewards = torch.zeros(self.n_envs, device=device)\n",
    "\n",
    "\n",
    "        for step_num in range(self.n_steps):\n",
    "            with torch.no_grad():\n",
    "                actions = agent.get_action(enc_obs, env_skills)\n",
    "\n",
    "            next_obs, rewards, dones, truncateds, _ = self.envs.step(actions.cpu().numpy())\n",
    "            next_obs = torch.tensor(next_obs, dtype=torch.float32, device=device).permute(0, 3, 1, 2)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "            not_done = ~(dones | truncateds)\n",
    "\n",
    "            # Store the transitions\n",
    "            self.replay_buffer.store(obs, env_skills, actions, rewards, next_obs, not_done)\n",
    "            obs = next_obs\n",
    "            with torch.no_grad():\n",
    "                enc_obs = encoder(obs)\n",
    "            total_rewards += rewards\n",
    "\n",
    "            # Resample skills only for environments where episode ended\n",
    "            done_mask = dones | truncateds\n",
    "            if done_mask.any() or step_num % 50 == 0: ## RESAMPLING SKILLS QUITE A LOT :)\n",
    "                new_skills = F.normalize(torch.randn(done_mask.sum(), n_skill, device=device), dim=1)\n",
    "                env_skills[done_mask] = new_skills\n",
    "\n",
    "\n",
    "        writer.add_scalar(\"stats/Rewards\", total_rewards.mean().item() / self.n_steps, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(state, encoder): # takes state and passes through CNNEncoder \n",
    "    if state.dim() == 4 and state.shape[1] == 3:\n",
    "        features = encoder(state)\n",
    "    elif state.dim() == 3 and state.shape[0] == 3:\n",
    "        state = state.unsqueeze(0)\n",
    "        features = encoder(state)\n",
    "    elif state.dim() == 2:\n",
    "        features = state\n",
    "    else:\n",
    "        raise ValueError(f'found a state with shape: {state.shape} in get_latent_representationss')\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1164da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ed6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillPolicy: # implements discrete SAC \n",
    "    def __init__(self, n_obs, n_skills, n_actions, representation, tau=0.005, lr=1e-4, gamma = 0.99, automatic_entropy_tuning=True, par_alpha=0.2, target_entropy=None):\n",
    "        \n",
    "        \n",
    "        self.alpha = par_alpha # 1.5\n",
    "        self.n_actions = n_actions\n",
    "        self.encoder = representation.encoder\n",
    "        self.representation = representation\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Q1Network\n",
    "        self.q1_net = nn.Sequential(\n",
    "            nn.Linear(n_obs + n_skills, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.n_actions)\n",
    "        ).to(device)\n",
    "        \n",
    "        # Q2Network\n",
    "        self.q2_net = copy.deepcopy(self.q1_net).to(device)\n",
    "\n",
    "        # Policy Network\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(n_obs + n_skills, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.n_actions)\n",
    "        ).to(device)\n",
    "\n",
    "        # Target Q1 Netowrk \n",
    "        self.q1_target_net = copy.deepcopy(self.q1_net).to(device)\n",
    "        \n",
    "        # Target Q2 network\n",
    "        self.q2_target_net = copy.deepcopy(self.q1_net).to(device)\n",
    "\n",
    "        # self.q1_target_net.load_state_dict(self.q1_net.state_dict()) # maybe faster\n",
    "        # self.q2_target_net.load_state_dict(self.q2_net.state_dict())\n",
    "\n",
    "        # Single Q optimizer \n",
    "        self.q_optimizer = Adam(\n",
    "            list(self.q1_net.parameters()) + list(self.q2_net.parameters()), lr=lr\n",
    "        )\n",
    "\n",
    "        # Policy Optimizer \n",
    "        self.policy_optimizer = Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "        # self.gamma = gamma\n",
    "\n",
    "        ###########   add temperature  ############\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "        if automatic_entropy_tuning:\n",
    "            # target entropy = −|A|\n",
    "            self.target_entropy = -self.n_actions if target_entropy is None else target_entropy\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "            self.alpha_optimizer = Adam([self.log_alpha], lr=lr)\n",
    "        else:\n",
    "            self.alpha = par_alpha\n",
    "        ################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############ Selection of the action  #############\n",
    "    def get_policy_distribution(self, states, skills):\n",
    "        states = encode_state(states, self.encoder)\n",
    "        inputs = torch.cat([states, skills], dim=-1)\n",
    "        logits = self.policy(inputs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def get_action(self, states, skills, eval=False):\n",
    "        dist = self.get_policy_distribution(states, skills)\n",
    "        if eval:\n",
    "            return torch.argmax(dist.probs, dim=-1)\n",
    "        return dist.sample()\n",
    "    ###################################################\n",
    "\n",
    "\n",
    "    # def get_entropy(self, states, skills):\n",
    "    #     dist = self.get_policy_distribution(states, skills)\n",
    "    #     return dist.entropy()\n",
    "\n",
    "\n",
    "    def get_q_loss(self, states, actions, rewards, next_states, not_dones, skills):\n",
    "\n",
    "        # ext_reward = rewards.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            phi = self.representation.get_latent_representation(states)  # phi(s) \n",
    "            phi_next = self.representation.get_latent_representation(next_states) # phi (s')\n",
    "\n",
    "            delta_phi = phi - phi_next\n",
    "            # skills = F.normalize(skills, p=2, dim=1)\n",
    "\n",
    "            intrinsic_reward = torch.einsum('bi,bi->b', delta_phi, skills).unsqueeze(-1)\n",
    "            intrinsic_reward = torch.clamp(intrinsic_reward, -10, 10)\n",
    "            # add extrinsic reward ??\n",
    "            \n",
    "            ############## target Q value #######################\n",
    "            next_states = encode_state(next_states, self.encoder)\n",
    "            next_input = torch.cat([next_states, skills], dim=-1)\n",
    "            next_q1 = self.q1_target_net(next_input)\n",
    "            next_q2 = self.q2_target_net(next_input)\n",
    "            next_q = torch.min(next_q1, next_q2)\n",
    "\n",
    "            \n",
    "            next_pi = self.get_policy_distribution(next_states, skills)\n",
    "            log_probs = next_pi.logits.log_softmax(dim=-1)\n",
    "            # next_entropy = next_pi.entropy().unsqueeze(-1)\n",
    "            # next_entropy = next_pi.logits.log_softmax(dim=-1)\n",
    "\n",
    "            if self.automatic_entropy_tuning:\n",
    "                alpha = self.log_alpha.exp()\n",
    "            else:\n",
    "                alpha = self.alpha\n",
    "\n",
    "            next_q_val = (next_pi.probs * (next_q - alpha * log_probs)).sum(dim=-1, keepdim=True)\n",
    "            q_target = intrinsic_reward + (self.gamma * not_dones * next_q_val) # + ext_reward # added extrinisic reward, may not make sense\n",
    "            #####################################################\n",
    "\n",
    "        ############  Q losses ##################\n",
    "        states = encode_state(states, self.encoder)\n",
    "        current_inputs = torch.cat([states, skills], dim=-1)\n",
    "        q1 = self.q1_net(current_inputs).gather(1, actions.long().unsqueeze(-1))\n",
    "        q2 = self.q2_net(current_inputs).gather(1, actions.long().unsqueeze(-1))\n",
    "\n",
    "        loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n",
    "        #######################################\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    #################### policy loss ##################\n",
    "    def get_policy_loss(self, states, skills):\n",
    "        dist = self.get_policy_distribution(states, skills)\n",
    "        probs = dist.probs\n",
    "        log_probs = dist.logits.log_softmax(dim=-1)\n",
    "\n",
    "        states = encode_state(states, self.encoder)\n",
    "\n",
    "        inputs = torch.cat([states, skills], dim=-1)\n",
    "        q1 = self.q1_net(inputs)\n",
    "        q2 = self.q2_net(inputs)\n",
    "        q = torch.min(q1, q2)\n",
    "\n",
    "        # added alpha \n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha = self.log_alpha.exp()\n",
    "        else:\n",
    "            alpha = self.alpha\n",
    "\n",
    "\n",
    "        policy_loss = -(probs * (q - (alpha * log_probs))).sum(dim=1).mean()\n",
    "        return policy_loss\n",
    "    #####################################################\n",
    "\n",
    "    ############### entropy loss ###################\n",
    "    def get_entropy_loss(self, states, skills):\n",
    "        dist = self.get_policy_distribution(states, skills)\n",
    "        # dist.entropy()\n",
    "        probs = dist.probs\n",
    "        log_probs = dist.logits.log_softmax(dim=-1)\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n",
    "        else:\n",
    "            alpha_loss = 0\n",
    "        return alpha_loss\n",
    "    # self.alpha_opt.zero_grad()\n",
    "    #         alpha_loss.backward()\n",
    "    #         self.alpha_opt.step()\n",
    "    ###################################################    \n",
    "\n",
    "\n",
    "    def update(self, replay_buffer, i):\n",
    "\n",
    "      for _ in range(1): # i dont think its good idea to set this too high ?\n",
    "        states, skills, actions, rewards, next_states, not_dones = replay_buffer.sample()\n",
    "        # Compute Q-loss\n",
    "        q_loss = self.get_q_loss(states, actions, rewards, next_states, not_dones, skills)\n",
    "        self.q_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "\n",
    "        # Update the policy\n",
    "        policy_loss = self.get_policy_loss(states, skills)\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "      # Soft update the target networks using Polyak averaging\n",
    "      self.soft_update(self.q1_net, self.q1_target_net)\n",
    "      self.soft_update(self.q2_net, self.q2_target_net)\n",
    "      # entropy = self.get_entropy(states, skills).mean().item()\n",
    "\n",
    "      alpha_loss = self.get_entropy_loss(states, skills)\n",
    "      self.alpha_optimizer.zero_grad()\n",
    "      alpha_loss.backward()\n",
    "      self.alpha_optimizer.step()\n",
    "\n",
    "      writer.add_scalar(\"loss/entropy_loss\", alpha_loss, i)\n",
    "\n",
    "      writer.add_scalar(\"loss/q_loss\", q_loss.item(), i)\n",
    "      writer.add_scalar(\"loss/ - policy loss\", -policy_loss.item(), i)\n",
    "\n",
    "\n",
    "    def soft_update(self, source, target):\n",
    "        \"\"\"Soft update the target network\"\"\"\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class CNNEncoder(nn.Module):\n",
    "#     \"\"\"\n",
    "#     CNN encoder loosely based on LeCun et al. (1989) but adapted for 3×210×160 inputs\n",
    "#     and with two 1024‑unit hidden layers in the fully‑connected head.\n",
    "    \n",
    "#     Input:  (B, 3, 210, 160)\n",
    "#     Output: (B, out_dim)\n",
    "#     \"\"\"\n",
    "#     def __init__(self, out_dim: int):\n",
    "#         super().__init__()\n",
    "#         self.conv_net = nn.Sequential(\n",
    "#             # C1: conv 3→32, 5×5, pad=2 → (32, 210, 160)\n",
    "#             nn.Conv2d(3, 32, kernel_size=5, padding=2),\n",
    "#             nn.Tanh(),\n",
    "#             # S2: avg pool 2×2       → (32, 105,  80)\n",
    "#             nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "#             # C3: conv 32→64, 5×5, pad=2 → (64, 105,  80)\n",
    "#             nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
    "#             nn.Tanh(),\n",
    "#             # S4: avg pool 2×2       → (64,  52,  40)\n",
    "#             nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "#             # C5: conv 64→128, 5×5, pad=2 → (128,  52,  40)\n",
    "#             nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
    "#             nn.Tanh(),\n",
    "#             # S6: avg pool 2×2       → (128,  26,  20)\n",
    "#             nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "#             nn.Flatten(),  # → (B, 128 * 26 * 20)\n",
    "#         )\n",
    "        \n",
    "#         # Compute flattened size dynamically\n",
    "#         with torch.no_grad():\n",
    "#             dummy = torch.zeros(1, 3, 210, 160)\n",
    "#             flat_size = self.conv_net(dummy).shape[1]  # should be 128*26*20 = 66560\n",
    "        \n",
    "#         # Fully‑connected head with 2 hidden layers of 1024 units each\n",
    "#         self.fc_head = nn.Sequential(\n",
    "#             nn.Linear(flat_size, 1024),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(1024, 1024),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(1024, out_dim)\n",
    "#         )\n",
    "        \n",
    "#         # Initialize weights as in LeNet (Xavier for tanh)\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "#                 nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = self.conv_net(x/255.0)    # conv → flatten\n",
    "#         return self.fc_head(x)  # two 1024‑unit hidden layers → out_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fc79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),  # (3, 210, 160) → (32, 52, 39)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # → (64, 25, 18)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # → (64, 23, 16)\n",
    "            nn.ReLU()\n",
    "        ).to(device)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 22 * 16, out_dim),\n",
    "            nn.ReLU()\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x / 255.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RepresentationFunction(nn.Module): # Meat of METRA\n",
    "    def __init__(self, n_obs, n_skill, lr=1e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encode input state \n",
    "        self.encoder = CNNEncoder(out_dim=256) # output of the CNN\n",
    "\n",
    "        # phi network - n_obs is hardcoded to 256 in main\n",
    "        self.representation_func = nn.Sequential(\n",
    "            nn.Linear(n_obs, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_skill)\n",
    "        ).to(device)\n",
    "\n",
    "        # joint optimization over encoder and phi\n",
    "        self.optimizer = Adam(list(self.parameters()) + list(self.encoder.parameters()), lr=1e-4)\n",
    "        \n",
    "        # lagrange mult\n",
    "        self.lambda_param = nn.Parameter(torch.tensor(30.0, requires_grad=True, device=device))\n",
    "        \n",
    "        # lambda optimizer \n",
    "        self.lambda_optimizer = Adam([self.lambda_param], lr=1e-4)\n",
    "        \n",
    "        # threshold for metra constraint \n",
    "        self.epsilon = 0.001 # 1.0\n",
    "\n",
    "\n",
    "    def get_latent_representation(self, state, normalize=False):\n",
    "        features = encode_state(state, self.encoder) # Runs CNNEncoder \n",
    "        x = self.representation_func(features) # phi output \n",
    "        if normalize:\n",
    "            return F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def update(self, replay_buffer, i):\n",
    "        \"\"\"\n",
    "        Updates the representation function φ and the Lagrange multiplier λ\n",
    "        based on skill-consistency and distance constraints.\n",
    "        \"\"\"\n",
    "        for _ in range(50): # 10\n",
    "\n",
    "          state, skill, action, reward, next_state, not_done  = replay_buffer.sample()\n",
    "\n",
    "          current_representations = self.get_latent_representation(state)    # φ(s)\n",
    "          next_representations = self.get_latent_representation(next_state)  # φ(s')\n",
    "\n",
    "          # Consistency loss term\n",
    "          consistency_term = torch.einsum('bi,bi->b', (next_representations - current_representations), skill)\n",
    "\n",
    "          # Distance penalty term (to enforce norm constraints)\n",
    "          diff_norm_squared = torch.norm(current_representations - next_representations, dim=1) ** 2\n",
    "          penalty_term = torch.minimum(\n",
    "              torch.tensor(self.epsilon, device=diff_norm_squared.device),\n",
    "              (1.0 - diff_norm_squared).clone()\n",
    "          )\n",
    "\n",
    "          # Representation loss\n",
    "          representation_loss = -(consistency_term + self.lambda_param * penalty_term).mean()\n",
    "\n",
    "          # Lambda loss (only on penalty term)\n",
    "          lambda_loss = (self.lambda_param * penalty_term.detach()).mean()\n",
    "\n",
    "          # Backprop: update \\phi\n",
    "          self.optimizer.zero_grad()\n",
    "          representation_loss.backward(retain_graph=True)\n",
    "          self.optimizer.step()\n",
    "\n",
    "          # Backprop: update \\lambda\n",
    "          self.lambda_optimizer.zero_grad()\n",
    "          lambda_loss.backward()\n",
    "          self.lambda_optimizer.step()\n",
    "          with torch.no_grad():\n",
    "                self.lambda_param.clamp_(min=0.0)\n",
    "\n",
    "        writer.add_scalar(\"loss/representation_loss\", representation_loss.item(), i)\n",
    "        writer.add_scalar(\"loss/  lambda_loss\", lambda_loss.item(), i)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_heatmap(frames, save_path):\n",
    "#     heatmap = None\n",
    "#     for frame in frames:\n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "#         # naive thresholding to isolate the player\n",
    "#         _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "#         if heatmap is None:\n",
    "#             heatmap = np.zeros_like(thresh, dtype=np.float32)\n",
    "#         heatmap += thresh.astype(np.float32)\n",
    "\n",
    "#     # Normalize and save heatmap\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.imshow(heatmap, cmap='hot', interpolation='nearest')\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()\n",
    "\n",
    "def compute_coverage(heatmap: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Given a 2D heatmap of visit counts, returns the fraction of pixels\n",
    "    that were ever visited (i.e. count>0) vs. the total area.\n",
    "    \"\"\"\n",
    "    # How many pixels were ever nonzero?\n",
    "    covered = np.count_nonzero(heatmap)\n",
    "    total   = heatmap.size\n",
    "    return covered / total\n",
    "\n",
    "def compute_pacman_heatmap(\n",
    "    frames,\n",
    "    save_path=None,\n",
    "    yellow_lower=(20, 100, 100),\n",
    "    yellow_upper=(40, 255, 255),\n",
    "    morph_kernel_size=3,\n",
    "    blur_ksize=(21,21),\n",
    "    overlay_alpha=0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a heatmap of Pac‑Man’s positions over a sequence of RGB frames.\n",
    "    \n",
    "    Args:\n",
    "      frames: list of (H,W,3) uint8 RGB arrays.\n",
    "      save_path: where to write the PNG.\n",
    "      yellow_lower/upper: HSV thresholds for Pac‑Man’s color.\n",
    "      morph_kernel_size: size for opening to remove noise.\n",
    "      blur_ksize: Gaussian blur kernel to smooth final heatmap.\n",
    "      overlay_alpha: how strongly to blend heatmap on top of the first frame.\n",
    "    \n",
    "    Returns:\n",
    "      heatmap: float32 array of shape (H,W) with counts of visits.\n",
    "      coverage: fraction of pixels ever visited.\n",
    "    \"\"\"\n",
    "    # prepare  \n",
    "    H, W, _ = frames[0].shape\n",
    "    visit_map = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    # set up morphological kernel\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (morph_kernel_size,)*2)\n",
    "\n",
    "    for frame in frames:\n",
    "        # 1) HSV color‐segment Pac‑Man\n",
    "        hsv  = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)\n",
    "        mask = cv2.inRange(hsv, np.array(yellow_lower), np.array(yellow_upper))\n",
    "\n",
    "        # 2) clean small speckles\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # 3) accumulate visit_map (binary) so repeated visits don’t accumulate linearly\n",
    "        visit_map += (mask > 0).astype(np.float32)\n",
    "\n",
    "    # 4) coverage: fraction of pixels ever visited\n",
    "    coverage = np.count_nonzero(visit_map) / visit_map.size\n",
    "\n",
    "    # 5) smooth for nicer appearance\n",
    "    smooth_map = cv2.GaussianBlur(visit_map, blur_ksize, sigmaX=0)\n",
    "\n",
    "    # 6) normalize to [0,1]\n",
    "    norm_map = smooth_map / (smooth_map.max() + 1e-8)\n",
    "\n",
    "    # 7) overlay on first frame\n",
    "    background = frames[0].astype(np.float32)/255.0\n",
    "    heatmap_rgb = plt.get_cmap('hot')(norm_map)[:,:,:3]  # H×W×3\n",
    "    overlay = (1-overlay_alpha)*background + overlay_alpha*heatmap_rgb\n",
    "\n",
    "    # 8) save\n",
    "    if save_path is not None:\n",
    "        plt.imsave(save_path, np.clip(overlay, 0, 1))\n",
    "\n",
    "    return visit_map, coverage\n",
    "\n",
    "\n",
    "def visualize_representation(replay_buffer, representation, global_step, n_samples=1000, folder='.', n_skills=10):\n",
    "    skill_data = []\n",
    "    for skill_id in range(n_skills):\n",
    "        samples = replay_buffer.sample_by_skill(skill_id, num_samples=max(int(n_samples/10), 100))\n",
    "        skill_data.append(samples)\n",
    "\n",
    "    # Combine all samples\n",
    "    states = torch.cat([s[0] for s in skill_data], dim=0)\n",
    "    skills = torch.cat([s[1] for s in skill_data], dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        skill_ids = torch.argmax(skills, dim=1)  # assume Gaussian skills aren't passed here!\n",
    "\n",
    "        phis = representation.get_latent_representation(states).cpu().numpy()\n",
    "        skill_ids_np = skill_ids.cpu().numpy()\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=20)\n",
    "    tsne_result = tsne.fit_transform(phis)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for skill_id in range(n_skills):\n",
    "        idx = skill_ids_np == skill_id\n",
    "        if np.sum(idx) == 0:\n",
    "            continue\n",
    "        plt.scatter(tsne_result[idx, 0], tsne_result[idx, 1], label=f'Skill {skill_id}', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.title('t-SNE of Representation φ(s)')\n",
    "    plt.savefig(f\"{folder}/representation_tsne.png\")\n",
    "    plt.close()\n",
    "\n",
    "    if len(np.unique(skill_ids_np)) > 1:\n",
    "        sil_score = silhouette_score(phis, skill_ids_np)\n",
    "        writer.add_scalar(\"representation/silhouette_score\", sil_score, global_step)\n",
    "\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(phis)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for skill_id in range(n_skills):\n",
    "        idx = skill_ids_np == skill_id\n",
    "        if not idx.any():\n",
    "            continue\n",
    "        plt.scatter(pca_result[idx, 0], pca_result[idx, 1], label=f'Skill {skill_id}', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.title('PCA of Representation φ(s)')\n",
    "    plt.savefig(f\"{folder}/pca_representation_pca.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_skills(env_name, policy, representation, global_step, writer=None, n_skills=5, steps_per_skill=512, video_dir='skill_videos'):\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "    temp_buffer = ReplayBuffer(capacity=steps_per_skill * n_skills)\n",
    "    temp_buffer.batch_size = 512\n",
    "\n",
    "    # combined_heatmap = np.zeros_like(heatmap)\n",
    "\n",
    "    for skill_id in range(n_skills): # For every z vector, I will perform ONLY the action related to that skill and get the reward\n",
    "        env = gym.make(env_name, render_mode='rgb_array')\n",
    "        obs, _ = env.reset()\n",
    "        # obs = torch.tensor(obs, dtype=torch.float32, device=device).view(1, -1) ## NO need to flatten CNN encoder in representation function will handle it\n",
    "        obs = torch.unsqueeze(torch.tensor(obs, dtype=torch.float32, device=device), 0).permute(0, 3, 1, 2)\n",
    "\n",
    "        # USING ONE HOT ENCODED SKILLS HERE SO THAT WE CAN FIND THE DIFFERENT SKILLS SEPARATELY\n",
    "        skill = torch.zeros(1, n_skills, device=device)\n",
    "        skill[0, skill_id] = 1.0  # One-hot vector for evaluation\n",
    "\n",
    "\n",
    "        frames = []\n",
    "        total_intrinsic_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        norms = []\n",
    "\n",
    "        \n",
    "        while step_count < steps_per_skill:\n",
    "            with torch.no_grad():\n",
    "                action = policy.get_action(obs, skill, eval=True).item()\n",
    "            next_obs, _, terminated, truncated, _ = env.step(action)\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "\n",
    "            # next_obs_tensor = torch.tensor(next_obs, dtype=torch.float32, device=device).view(1, -1)\n",
    "            next_obs_tensor = torch.unsqueeze(torch.tensor(next_obs, dtype=torch.float32, device=device), 0).permute(0, 3, 1, 2) ## Encoding doesn't matter because get_latent_representation encodes\n",
    "            delta_phi = representation.get_latent_representation(next_obs_tensor) - representation.get_latent_representation(obs)\n",
    "            norms.append(delta_phi.norm(dim=1))  \n",
    "            intrinsic_reward = torch.einsum('bi,bi->b', delta_phi, skill).item()\n",
    "            temp_buffer.store(obs, skill, torch.tensor([action]), torch.tensor([intrinsic_reward]), next_obs_tensor, torch.tensor([~(terminated or truncated)]))\n",
    "            total_intrinsic_reward += intrinsic_reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            obs = next_obs_tensor\n",
    "            step_count += 1\n",
    "\n",
    "        env.close()\n",
    "        \n",
    "        heatmap_path = os.path.join(video_dir, f\"skill_{skill_id}_coverage.png\") \n",
    "        heatmap, coverage = compute_pacman_heatmap(frames)\n",
    "        plt.imsave(heatmap_path, heatmap / heatmap.max(), cmap='hot')\n",
    "        if writer:\n",
    "            writer.add_scalar(f\"eval/coverage_skill_{skill_id}\", coverage, global_step)\n",
    "        if skill_id == 0:\n",
    "            combined_heatmap = (heatmap > 0).astype(np.float32)\n",
    "        else:\n",
    "            combined_heatmap += (heatmap > 0).astype(np.float32)\n",
    "\n",
    "        video_path = os.path.join(video_dir, f\"skill_{skill_id}.mp4\")\n",
    "        imageio.mimsave(video_path, frames, fps=30)\n",
    "\n",
    "        # writer.add_scalar(\"representation/mean_delta_phi_norm\", delta_phi.norm(dim=1).item(), global_step)\n",
    "        mean_norm = torch.cat(norms).mean().item()\n",
    "        writer.add_scalar(\"representation/mean_delta_phi_norm\", mean_norm, global_step)\n",
    "        # heatmap_path = os.path.join(video_dir, f\"skill_{skill_id}_heatmap.png\")\n",
    "        # compute_pacman_heatmap(frames, heatmap_path)\n",
    "\n",
    "        del frames\n",
    "        torch.cuda.empty_cache()  \n",
    "        # print(f\"Skill {skill_id}: steps = {step_count}, intrinsic reward = {total_intrinsic_reward:.2f}, saved to {video_path}\")\n",
    "        if writer:\n",
    "            writer.add_scalar(f\"eval/intrinsic_reward_skill_{skill_id}\", total_intrinsic_reward, global_step)\n",
    "            writer.add_scalar(f\"eval/episode_length_skill_{skill_id}\", step_count, global_step)\n",
    "    \n",
    "    # visualize_representation(temp_buffer, representation, folder=video_dir, n_skills=n_skills)\n",
    "\n",
    "    visualize_representation(temp_buffer, representation, global_step, folder=video_dir, n_skills=n_skills)\n",
    " \n",
    "    ## Evaluate action distribution -- irrespective of the skill\n",
    "    states, skills, _, _, _, _ = temp_buffer.sample()\n",
    "    with torch.no_grad():\n",
    "        dist  = policy.get_policy_distribution(states, skills)\n",
    "        probs = dist.probs\n",
    "    mean_probs = probs.mean(dim=0)\n",
    "\n",
    "    for a in range(9): ## 9 actions in Ms. Pacman\n",
    "        writer.add_scalar(f\"action_dist/action_{a}\", mean_probs[a].item())\n",
    "    writer.add_histogram(\"action_dist/all_actions\", mean_probs.cpu().numpy())\n",
    "\n",
    "\n",
    "    # aggregate across all skills by poooling heatmap\n",
    "    \n",
    "    \n",
    "    combined_coverage = compute_coverage(combined_heatmap)\n",
    "    writer.add_scalar(\"eval/combined_coverage\", combined_coverage, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41c70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n",
      "/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "EPOCH: 2\n",
      "EPOCH: 3\n",
      "EPOCH: 4\n",
      "EPOCH: 5\n",
      "EPOCH: 6\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 2 has a total capacity of 44.45 GiB of which 50.00 MiB is free. Including non-PyTorch memory, this process has 44.39 GiB memory in use. Of the allocated memory 38.72 GiB is allocated by PyTorch, and 5.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m drl\u001b[38;5;241m.\u001b[39mrollout(policy, epoch, n_skill, representation\u001b[38;5;241m.\u001b[39mencoder)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mrepresentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# update phi and lambda \u001b[39;00m\n\u001b[1;32m     50\u001b[0m policy\u001b[38;5;241m.\u001b[39mupdate(drl\u001b[38;5;241m.\u001b[39mreplay_buffer, epoch) \u001b[38;5;66;03m# update policy with scratch implemented discrete SAC \u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m## EVALUATING SKILL\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 48\u001b[0m, in \u001b[0;36mRepresentationFunction.update\u001b[0;34m(self, replay_buffer, i)\u001b[0m\n\u001b[1;32m     45\u001b[0m state, skill, action, reward, next_state, not_done  \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     47\u001b[0m current_representations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_latent_representation(state)    \u001b[38;5;66;03m# φ(s)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m next_representations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_latent_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# φ(s')\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Consistency loss term\u001b[39;00m\n\u001b[1;32m     51\u001b[0m consistency_term \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi,bi->b\u001b[39m\u001b[38;5;124m'\u001b[39m, (next_representations \u001b[38;5;241m-\u001b[39m current_representations), skill)\n",
      "Cell \u001b[0;32mIn[20], line 31\u001b[0m, in \u001b[0;36mRepresentationFunction.get_latent_representation\u001b[0;34m(self, state, normalize)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_latent_representation\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 31\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mencode_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Runs CNNEncoder \u001b[39;00m\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_func(features) \u001b[38;5;66;03m# phi output \u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize:\n",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m, in \u001b[0;36mencode_state\u001b[0;34m(state, encoder)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_state\u001b[39m(state, encoder): \u001b[38;5;66;03m# takes state and passes through CNNEncoder \u001b[39;00m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m         features \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m      5\u001b[0m         state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m, in \u001b[0;36mCNNEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/modules/activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/mlobo6/miniconda3/envs/drl/lib/python3.10/site-packages/torch/nn/functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 2 has a total capacity of 44.45 GiB of which 50.00 MiB is free. Including non-PyTorch memory, this process has 44.39 GiB memory in use. Of the allocated memory 38.72 GiB is allocated by PyTorch, and 5.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env = gym.make('ALE/MsPacman-v5')\n",
    "# n_obs = env.observation_space.shape[0] * env.observation_space.shape[1] * env.observation_space.shape[2]\n",
    "n_obs = 256 # Encoded dimension\n",
    "n_actions = env.action_space.n # 9\n",
    "n_skill = 10  # 0 - NOOP, 1 - UP, 2 - LEFT, 3 - RIGHT, 4 - DOWN\n",
    "\n",
    "representation = RepresentationFunction(n_obs, n_skill) # phi\n",
    "policy = SkillPolicy(n_obs, n_skill, n_actions, representation) # pi for dicrete SAC \n",
    "\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "# recovering from a failed run\n",
    "chkpt_path = ''\n",
    "if len(chkpt_path) > 0:\n",
    "    if not os.path.exists(chkpt_path):\n",
    "        raise ValueError(\"Load folder path doesn't exist\")\n",
    "    checkpoint = torch.load(chkpt_path)\n",
    "    policy.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "    policy.q1_net.load_state_dict(checkpoint['q1_state_dict'])\n",
    "    policy.q2_net.load_state_dict(checkpoint['q2_state_dict'])\n",
    "    policy.q1_target_net.load_state_dict(checkpoint['q1_target_state_dict'])\n",
    "    policy.q2_target_net.load_state_dict(checkpoint['q2_target_state_dict'])\n",
    "    policy.policy_optimizer.load_state_dict(checkpoint['policy_optimizer_state_dict'])\n",
    "    policy.q_optimizer.load_state_dict(checkpoint['q_optimizer_state_dict'])\n",
    "\n",
    "    representation.representation_func.load_state_dict(checkpoint['representation_state_dict'])\n",
    "    representation.optimizer.load_state_dict(checkpoint['representation_optimizer_state_dict'])\n",
    "    representation.lambda_param = torch.tensor(checkpoint['lambda_param'], requires_grad=True)\n",
    "    representation.lambda_optimizer.load_state_dict(checkpoint['lambda_optimizer_state_dict'])\n",
    "\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 551\n",
    "\n",
    "drl = DRL(buffer_size = 10000)\n",
    "\n",
    "if not os.path.exists(f\"./model/{timestamp}\"):\n",
    "    os.makedirs(f\"./model/{timestamp}\")\n",
    "\n",
    "folder = f'./model/{timestamp}'\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + start_epoch):\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    drl.rollout(policy, epoch, n_skill, representation.encoder)\n",
    "    \n",
    "    representation.update(drl.replay_buffer, epoch) # update phi and lambda \n",
    "    policy.update(drl.replay_buffer, epoch) # update policy with scratch implemented discrete SAC \n",
    "    if epoch % 10 == 0:\n",
    "        ## EVALUATING SKILL\n",
    "        evaluate_skills('ALE/MsPacman-v5', policy, representation,\\\n",
    "                epoch, writer=writer, n_skills=n_skill, video_dir=f'{folder}/video_eval_{epoch}') ## ONLY doing for the first 3 skills\n",
    "\n",
    "        ## EVALUATING REPRESENTATION FUNCTION\n",
    "        # visualize_representation(replay_buffer=drl.replay_buffer,\\\n",
    "        #      representation=representation, folder=f\"{folder}/video_eval_{epoch}\") ## DOING THIS IN SkILL EVALUATION WHICH DOES ONE HOT ENCODED SKILLS\n",
    "        # writer.add_image(\"eval/tSNE\", \\\n",
    "        #      torch.tensor(np.array(Image.open(f\"{folder}/video_eval_{epoch}/representation_tsne.png\"))).permute(2, 0, 1), epoch)\n",
    "\n",
    "        gc.collect()\n",
    "    cleanup_memory()\n",
    "\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'policy_state_dict': policy.policy.state_dict(),\n",
    "    'q1_state_dict': policy.q1_net.state_dict(),\n",
    "    'q2_state_dict': policy.q2_net.state_dict(),\n",
    "    'q1_target_state_dict': policy.q1_target_net.state_dict(),\n",
    "    'q2_target_state_dict': policy.q2_target_net.state_dict(),\n",
    "    'policy_optimizer_state_dict': policy.policy_optimizer.state_dict(),\n",
    "    'q_optimizer_state_dict': policy.q_optimizer.state_dict(),\n",
    "\n",
    "    'representation_state_dict': representation.representation_func.state_dict(),\n",
    "    'representation_optimizer_state_dict': representation.optimizer.state_dict(),\n",
    "    'lambda_param': representation.lambda_param.detach().item(),\n",
    "    'lambda_optimizer_state_dict': representation.lambda_optimizer.state_dict(),\n",
    "}, f\"./model/{timestamp}/checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e22a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00683083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cbbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "drl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
